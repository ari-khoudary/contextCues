---
title: "visualizing individual subjects data"
output: 
  github_document:
    toc: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

this notebook is devoted to investigating how individual subjects performed during calibration, learning, and inference. key questions that it aims to address are:

1. what do the staircases look like during calibration?
2. what coherence levels did each subject observe for each stimulus?
3. how accurately were they making button presses during learning?
4. what do their RTs look like during learning?
5. are the learned choice probabilities reflected in decision accuracy?

```{r load packages, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(tidyverse)
library(patchwork)
library(lme4)
library(lmerTest)
library(emmeans)
library(knitr)

# potentially useful in the future
library(performance) # for easy model comparisons
# library(jtools) # for alternative model summaries
library(sjPlot) # for formatting model result tables
# library(MetBrewer) # for pretty art-inspired color palettes
```


```{r load & tidy calibration data, include=FALSE}
# get info about which staircase generated which coherence
staircase_df <- read.csv('../../data/tidied/staircases.csv') %>% 
  pivot_longer(cols = c(stair1, stair2), names_to = 'staircase') %>%
  rename(coherence = value) %>%
  mutate(coherence = round(coherence, 2)) 

# load calibrated coherences
calibrated_coherence <- read.csv('../../data/tidied/calibrated_coherence.csv') %>%
  mutate(img1Coh = round(img1, 2),
         img2Coh = round(img2, 2))

col_order <- c('subID', 'img1Path', 'img1Coh', 'img2Path', 'img2Coh')
calibrated_coherence <- calibrated_coherence[, col_order]
```


## calibration data

### what do the staircases look like during calibration?
solid line indicates the level of decision accuracy we are calibrating to.
```{r plot calibration data, echo=F, fig.height=10, fig.width=6, fig.dpi=300}

staircase_df %>%
  mutate(subID = paste0('sub', subID),
         targetIdx = paste0('img', imgPath)) %>% 
  ggplot(aes(x=trial, y=coherence, color=staircase)) +
  facet_wrap(subID ~ targetIdx, ncol=4) +
  geom_line(linewidth=1) +
  theme_bw() +
  geom_hline(yintercept = 0.7)

```

### what coherence levels did each subject observe for each stimulus?
coherence values that obtained 70% accuracy for each image for each subject:
```{r display coherence for each subject, echo=F, out.width='350px', fig.align='center'}

kable(calibrated_coherence)
knitr::include_graphics("../../imageGrid.png")

```

***

## learning data
```{r load & tidy learning data, include=F}
learning_files = list.files('../../data', full.names = TRUE, pattern='block1_learning.csv', recursive = T)
learning_keep_idx <- grep('excluded', learning_files, invert=T)
learning_keep_idx <- learning_keep_idx
learning_df = do.call(rbind, lapply(learning_files[learning_keep_idx], function(x) { read.csv(x, header = TRUE)} )) 

# transform RTs
learning_df <- learning_df %>%
  group_by(subID) %>%
  mutate(logRT = log(RT),
         zlogRT = scale(logRT),
         cueLabel = factor(cueIdx, levels=c(3,1,2), labels=c('50% A/B', '80% A', '80% B')))
```

### how accurately were they making button presses during learning?
```{r learning accuracy, echo = F, fig.height=3, fig.width=10}
learning_df %>%
  mutate(imageIdx = factor(imageIdx)) %>% 
  ggplot(aes(x=imageIdx, y=accuracy)) +
  theme_bw() +
  facet_grid(~ subID) +
  stat_summary(fun='mean', geom='point', size=3) +
  geom_hline(yintercept = 0.8, linetype='dashed') +
  scale_y_continuous(n.breaks=6) +
  theme(text = element_text(size=14))
```

### what do RTs look like during learning?
```{r learning RTs, echo = F, fig.dpi=300, fig.height=2, fig.width=5}

# plot correlation coefficients over the course of learning 
bar1 <- learning_df %>%
  filter(trial < 20) %>%
  group_by(cueLabel) %>%
  summarise(y=cor(trial, zlogRT, use='na.or.complete')) %>%
  ggplot(aes(cueLabel, y, color=cueLabel, fill = cueLabel)) +
  geom_col() + geom_hline(yintercept=0) +
  theme_bw() + ylim(-0.3, 0.35) +
  labs(y='correlation coefficient') + theme(legend.position='none')

bar2 <- learning_df %>%
  filter(trial >= 20 & trial < 38) %>%
  group_by(cueLabel) %>%
  summarise(y=cor(trial, zlogRT, use='na.or.complete')) %>%
  ggplot(aes(cueLabel, y, color=cueLabel, fill = cueLabel)) +
  geom_col() + geom_hline(yintercept = 0) +
  theme_bw() + ylim(-0.3, 0.35) +
  labs(y='correlation coefficient') + theme(legend.position='none')

bar3 <- learning_df %>%
  filter(trial >= 38 & trial < 57) %>%
  group_by(cueLabel) %>%
  summarise(y=cor(trial, zlogRT, use='na.or.complete')) %>%
  ggplot(aes(cueLabel, y, color=cueLabel, fill = cueLabel)) +
  geom_col() + geom_hline(yintercept = 0) +
  theme_bw() + ylim(-0.3, 0.35) +
  labs(y='correlation coefficient') + theme(legend.position='none')

bar4 <- learning_df %>%
  filter(trial >= 57) %>%
  group_by(cueLabel) %>%
  summarise(y=cor(trial, zlogRT, use='na.or.complete')) %>%
  ggplot(aes(cueLabel, y, color=cueLabel, fill = cueLabel)) +
  geom_col() + geom_hline(yintercept = 0) +
  theme_bw() + ylim(-0.3, 0.35) +
  labs(y='correlation coefficient') + theme(legend.position='none')

# plot group regressions
reg1 <- learning_df %>%
  filter(trial < 20) %>%
  ggplot(aes(x=trial, y=zlogRT, color=cueLabel)) +
  theme_bw() +
  facet_wrap(~ cueLabel) +
  geom_hline(yintercept = 0, linetype='dashed') +
  geom_point(alpha=0.25, size=1) +
  geom_smooth(method = 'lm', linewidth=1) + theme(legend.position='none')

reg2 <- learning_df %>%
  filter(trial >= 20 & trial < 38) %>%
  ggplot(aes(x=trial, y=zlogRT, color=cueLabel)) +
  theme_bw() +
  facet_wrap(~ cueLabel) +
  geom_hline(yintercept = 0, linetype='dashed') +
  geom_point(alpha=0.25, size=1) +
  geom_smooth(method = 'lm', linewidth=1) + theme(legend.position='none')

reg3 <- learning_df %>%
  filter(trial >= 38 & trial < 57) %>%
  ggplot(aes(x=trial, y=zlogRT, color=cueLabel)) +
  theme_bw() +
  facet_wrap(~ cueLabel) +
  geom_hline(yintercept = 0, linetype='dashed') +
  geom_point(alpha=0.25, size=1) +
  geom_smooth(method = 'lm', linewidth=1) + theme(legend.position='none')

reg4 <- learning_df %>%
  filter(trial >= 57) %>%
  ggplot(aes(x=trial, y=zlogRT, color=cueLabel)) +
  theme_bw() +
  facet_wrap(~ cueLabel) +
  geom_hline(yintercept = 0, linetype='dashed') +
  geom_point(alpha=0.25, size=1) +
  geom_smooth(method = 'lm', linewidth=1) + theme(legend.position='none')

bar1 + reg1 + plot_layout(widths=c(0.5, 1)) + plot_annotation(title = 'first quartile')  & 
  theme(text=element_text(size=8))

bar2 + reg2 + plot_layout(widths=c(0.5, 1)) + plot_annotation(title = 'second quartile') & 
   theme(text=element_text(size=8))

bar3 + reg3 + plot_layout(widths=c(0.5, 1)) + plot_annotation(title = 'third quartile') &
   theme(text=element_text(size=8))

bar4 + reg4 + plot_layout(widths=c(0.75, 1)) + plot_annotation(title = 'fourth quartile') &
   theme(text=element_text(size=8))
```

***

## inference data

```{r load in inference data & psq, echo=F}
inference_files = list.files('../../data', full.names = TRUE, pattern='block1_inference.csv', recursive = T)
inference_keep_idx <- grep('excluded', inference_files, invert=T)
inference_df = do.call(rbind, lapply(inference_files[inference_keep_idx], function(x) { read.csv(x, header = TRUE)} )) 
psq <- read.csv('../../data/tidied/subjective_probabilities.csv')

inference_df <- inference_df %>%
  mutate(cueIdx = case_when(targetIdx==1 & congruent==1 ~ 1,
                            targetIdx==2 & congruent==1 ~ 2,
                            targetIdx==1 & congruent==0 ~ 2,
                            targetIdx==2 & congruent==0 ~ 1,
                            is.na(congruent) ~ 3),
         cueLabel = factor(cueIdx, levels=c(3,1,2), labels=c('50% A/B', '80%A', '80%B')),
         cue_string = trimws(cue_string)) %>%
  left_join(., psq) %>% 
  mutate()


```

### what can learning RTs for each cue tell us about how those cues affect decision performance?
```{r individual learnRTs + decision accuracy, echo=F, fig.height=15, fig.width=8, fig.dpi=300}

RTs <- learning_df %>%
  ggplot(aes(x=trial, y=zlogRT, color=cueLabel)) +
  theme_bw() +
  geom_hline(yintercept=0, linetype='dashed') +
  geom_point(alpha=0.5, size=1) +
  geom_smooth(method = 'lm', linewidth=1.5) +
  facet_grid(subID ~ cueLabel) +
  scale_x_continuous(n.breaks=4) +
  labs(title = 'RTs over learning')
  
accuracy <- inference_df %>%
  ggplot(aes(x=cueLabel, y=accuracy, color=cueLabel)) +
  theme_bw() +
  facet_grid(rows='subID') +
  geom_hline(yintercept = 0.5, linetype='dashed') +
  stat_summary(fun.data='mean_se', geom='pointrange') +
  labs(y = 'proportion correct', title = 'inference accuracy') +
  scale_y_continuous(n.breaks=5)

estimates <- inference_df %>%
  ggplot(aes(x=cueLabel, y=estimate, color=cueLabel, fill=cueLabel)) +
  theme_bw() +
  facet_grid(rows = 'subID') +
  geom_hline(yintercept = 0.5, linetype = 'dashed') +
  geom_point() +
  labs(y = 'estimate', title = 'estimated probability') +
  scale_y_continuous(n.breaks=5)

estimates + accuracy + RTs + plot_layout(guides = 'collect', widths=c(0.5, 0.5, 1))
```

### are choices better predicted by true or estimated cue probabilities?
```{r group regressions: accuracy, echo=F, fig.dpi=300, fig.position='center', fig.width=8, fig.height=4}

# predicting accuracy based on cue level
m_cue <- glm(accuracy ~ cueLabel, inference_df, family='binomial',
         contrasts = list(cueLabel=contr.sum))
cue_df <- emmip(m_cue,  ~ cueLabel, type='response', CIs=T, plotit=F)

# predicting accuracy based on verbal estimate
m_estimate <- glm(accuracy ~ estimate, inference_df, family='binomial')
estimate_df <- emmip(m_estimate, ~ estimate, type='response', CIs=T, plotit=F,
                     at = list(estimate = quantile(inference_df$estimate, names=F, na.rm=T)))

# plotting results
p1 <- cue_df %>%
  ggplot(aes(cueLabel, yvar, color=cueLabel, fill=cueLabel)) +
  geom_hline(yintercept = 0.5, linetype='dashed') +
  geom_col(alpha=0, linewidth=1.5) +
  geom_errorbar(aes(ymin=LCL, ymax=UCL), color='black', width=0.1) +
  geom_jitter(aes(x=cueLabel, y=accuracy), color='gray', data=inference_df, height=0.05, size=0.5) +
  theme_bw() +
  ylim(0, 1) +
  labs(y = 'estimated p(correct)', title = 'objective cue probabilities',
       subtitle = 'glm(accuracy ~ cueLabel)')

p2 <- estimate_df %>%
  ggplot(aes(estimate, yvar)) +
  geom_hline(yintercept = 0.5, linetype='dashed') +
  geom_line() +
  geom_ribbon(aes(ymin=LCL, ymax=UCL), alpha=0.2) +
  geom_jitter(data=inference_df, aes(y=accuracy, color=cueLabel), height = 0.05, width=0.05, size=0.5) +
  theme_bw() +
  labs(y = 'estimated p(correct)', title = 'estimated cue probabilities',
       subtitle = 'glm(accuracy ~ estimate)')

p1 + p2 + plot_layout(guides='collect')
```

#### model comparison metrics
```{r model comparisons}

knitr::kable(compare_performance(m_cue, m_estimate, metrics=c('AIC', 'BIC', 'RMSE', 'Log_loss')))

```